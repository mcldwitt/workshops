{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-vJaSY3PFui"
      },
      "source": [
        "# <font color='darkorange'> Welcome to the reinforcement learning workshop!\n",
        "\n",
        "To run a cel click on the play button next to it\n",
        "\n",
        "<font color='red'>do not forget to run the first cell!</font> This will import all the necesary functions and modules.  \n",
        "This should take about 45 seconds to one minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vh8VQSgYNCHc"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from base64 import b64encode\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fO8wMVO8QDhb"
      },
      "source": [
        "# <font color='deeppink'> Here, we are creating the environment to work with.</font>  \n",
        "More documentation on the environment can be found here:  \n",
        "https:# www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "26Sr904INOQe"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zMhi0eRRpEC"
      },
      "source": [
        "# <font color='deeppink'> Let's have a first try at manipulating the environment.</font>   \n",
        "With the function `env.action_space.sample()` the environment will randomly choose to push the cart to the left or to the right. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F4Tyc88U00tZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 [ 0.02534623 -0.1515125  -0.03922006  0.3015787 ] 1.0 False {}\n",
            "step 1 [ 0.02231598 -0.34605417 -0.03318849  0.58163905] 1.0 False {}\n",
            "step 2 [ 0.01539489 -0.5406957  -0.02155571  0.863685  ] 1.0 False {}\n",
            "step 3 [ 0.00458098 -0.73551774 -0.00428201  1.1495132 ] 1.0 False {}\n",
            "step 4 [-0.01012938 -0.5403401   0.01870826  0.8554906 ] 1.0 False {}\n",
            "step 5 [-0.02093618 -0.735712    0.03581807  1.153997  ] 1.0 False {}\n",
            "step 6 [-0.03565042 -0.541075    0.05889801  0.8727571 ] 1.0 False {}\n",
            "step 7 [-0.04647192 -0.7369463   0.07635315  1.1833605 ] 1.0 False {}\n",
            "step 8 [-0.06121084 -0.5428935   0.10002036  0.9155554 ] 1.0 False {}\n",
            "step 9 [-0.07206871 -0.34925607  0.11833147  0.65590787] 1.0 False {}\n",
            "step 10 [-0.07905383 -0.54580927  0.13144962  0.98338425] 1.0 False {}\n",
            "step 11 [-0.08997002 -0.35266998  0.15111731  0.7347097 ] 1.0 False {}\n",
            "step 12 [-0.09702342 -0.15992278  0.16581151  0.4931434 ] 1.0 False {}\n",
            "step 13 [-0.10022188  0.0325201   0.17567438  0.25696373] 1.0 False {}\n",
            "step 14 [-0.09957147 -0.1646179   0.18081366  0.5995025 ] 1.0 False {}\n",
            "step 15 [-0.10286383 -0.36174732  0.1928037   0.9432461 ] 1.0 False {}\n",
            "step 16 [-0.11009878 -0.55887043  0.21166863  1.2897859 ] 1.0 True {}\n",
            "step 17 [-0.12127618 -0.36697307  0.23746434  1.070131  ] 0.0 True {}\n",
            "step 18 [-0.12861565 -0.56431216  0.25886697  1.426993  ] 0.0 True {}\n",
            "step 19 [-0.13990189 -0.37314713  0.2874068   1.2250595 ] 0.0 True {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\marie.dewitte\\Anaconda3\\envs\\Reinforcement_learning\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "\n",
        "for i in range(20):\n",
        "\n",
        "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
        "  observation, reward, done, info = env.step(env.action_space.sample())\n",
        "\n",
        "  print(\"step\", i, observation, reward, done, info)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z90VxfRS2F1"
      },
      "source": [
        "This results in the printed states, however this is not that interpretable,  \n",
        "so we are going to take a video of the training process and see what happens. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qGZWJCxFH8WU"
      },
      "outputs": [],
      "source": [
        "# returns an initial observation\n",
        "env.reset()\n",
        "for i in range(200):\n",
        "  # rendering and capturing the environment\n",
        "  env.render()\n",
        "\n",
        "  # env.action_space.sample() produces either 0 (left) or 1 (right).\n",
        "  observation, reward, done, info = env.step(env.action_space.sample())\n",
        "\n",
        "\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jZzUoCq3Buk"
      },
      "source": [
        "# <font color='deeppink'> Simple strategies to solve the environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RqtbeKui2IGJ"
      },
      "source": [
        "There are more 'naive' was of trying to solve this problem.  \n",
        "For example you could look at the angle of the pole and according to this push the cart left or right .\n",
        "\n",
        "Try to implement the following code: (got any questions? Ask your workshop teacher ;) )\n",
        "\n",
        "\n",
        "<img src=\"Capture1.PNG\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R3TbIwNDw1IA"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "angle = state[2]\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render()\n",
        "    \n",
        "\n",
        "    # insert the code here\n",
        "\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DAN_-_Ac2c2I"
      },
      "source": [
        "Or you could try to compensate for the difference in the pole angle.  \n",
        "In other words, push the pole to the left when the difference in the pole angle is negative,  \n",
        "and push the pole to the right if the pole angle is positive.\n",
        "\n",
        "Try to implement the following code: \n",
        "\n",
        "<img src=\"Capture.PNG\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HloUep_nOvoE"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "previous_angle = state[2]\n",
        "angle = state[2]\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    env.render(mode = 'rgb_array')\n",
        "\n",
        "\n",
        "    # insert the code here\n",
        "\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NsxQJbGaoau"
      },
      "source": [
        "# <font color=deeppink> Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4hGsyKgdyZy"
      },
      "source": [
        "Let's start by importing some additional libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vYBLb58zcjDw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37SxFb2Cd5-B"
      },
      "source": [
        "This cell contain the parameters that were discussed earlier.\n",
        "\n",
        "*   The learning rate\n",
        "*   Epsilon\n",
        "*   Epsilon decay \n",
        "*   Discount \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The number of episodes is the amount of time the environment is going to be run. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uXdYGsD7csMF"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.5 # waarde tussen 0 en 1\n",
        "\n",
        "DISCOUNT = 0.5 # waarde tussen 0 en 1\n",
        "EPISODES = 5000 # kan zeer hoog (denk in grootteorde 100 000), maar probeer het eerst met een lager getal\n",
        "total = 0\n",
        "total_reward = 0\n",
        "prior_reward = 0\n",
        "\n",
        "Observation = [30, 30, 50, 50]\n",
        "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "\n",
        "epsilon = 0.5 # waarde tussen 0 en 1\n",
        "\n",
        "epsilon_decay_value = 0.5 # Waarde tussen 0 en 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT_JN0nee3h5"
      },
      "source": [
        "For Q-learning, it is important that the state space is a discrete one,  \n",
        "however the cart position and pole angle are continueus values.  \n",
        "So let's fix this issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_KdrJkome2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30, 30, 50, 50, 2)\n"
          ]
        }
      ],
      "source": [
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "print(q_table.shape)\n",
        "\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = state/np_array_win_size+ np.array([12,6,1,6])\n",
        "    return tuple(discrete_state.astype(np.int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL1GXUvUfNHM"
      },
      "source": [
        "<font color=deeppink> Next step is to train the q-table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZvsgsgAWUfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\n",
            "Mean Reward: 0.023\n",
            "Mean Reward: 23.733\n",
            "Episode: 2000\n",
            "Mean Reward: 26.043\n",
            "Mean Reward: 29.419\n",
            "Episode: 4000\n",
            "Mean Reward: 33.38\n",
            "Mean Reward: 32.735\n",
            "Episode: 6000\n",
            "Mean Reward: 33.761\n",
            "Mean Reward: 33.615\n",
            "Episode: 8000\n",
            "Mean Reward: 33.756\n",
            "Mean Reward: 35.342\n",
            "Episode: 10000\n",
            "Mean Reward: 35.593\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for episode in range(EPISODES + 1): #go through the episodes\n",
        "    discrete_state = get_discrete_state(env.reset()) #get the discrete start for the restarted environment \n",
        "    done = False\n",
        "    episode_reward = 0 #reward starts as 0 for each episode\n",
        "\n",
        "    if episode % 2000 == 0: \n",
        "        print(\"Episode: \" + str(episode))\n",
        "\n",
        "    while not done: \n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "\n",
        "            action = np.argmax(q_table[discrete_state]) # take cordinated action\n",
        "        else:\n",
        "\n",
        "            action = np.random.randint(0, env.action_space.n) # do a random ation\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action) # step action to get new states, reward, and the \"done\" status.\n",
        "\n",
        "        episode_reward += reward # add the reward\n",
        "\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % 2000 == 0: # render\n",
        "            env.render()\n",
        "\n",
        "        if not done: # update q-table\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    if epsilon > 0.05: # epsilon modification\n",
        "        if episode_reward > prior_reward and episode > 10000:\n",
        "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
        "\n",
        "            if episode % 500 == 0:\n",
        "                print(\"Epsilon: \" + str(epsilon))\n",
        "\n",
        "\n",
        "    total_reward += episode_reward # episode total reward\n",
        "    prior_reward = episode_reward\n",
        "\n",
        "    if episode % 1000 == 0: # every 1000 episodes print the average time and the average reward\n",
        "        mean_reward = total_reward / 1000\n",
        "        print(\"Mean Reward: \" + str(mean_reward))\n",
        "        total_reward = 0\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SFNZsxsOax3i"
      },
      "source": [
        "<font color=\"deepskyblue\"> <font color=\"deepskyblue\"> Huge thank you to this author who provided the base of the code for q learning  \n",
        "https:# medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi25D5Nt_mYM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
